# app/llm.py

import os
import json
from openai import AzureOpenAI
from typing import Dict, Any
from .prompts import QUERY_SYS, FEWSHOTS, SUMM_SYS
from dotenv import load_dotenv

# ---------------------- Environment Setup ---------------------- #
load_dotenv()

# Azure OpenAI Client Initialization
client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

MODEL_DSL = os.getenv("AZURE_OPENAI_COMPLETION_DEPLOYMENT", "gpt-4o-mini")
MODEL_SUM = os.getenv("AZURE_OPENAI_COMPLETION_DEPLOYMENT", "gpt-4o-mini")


def build_messages(nl_query: str) -> list:
    """
    Build the messages required for LLM completion.

    Args:
        nl_query (str): The natural language query from the user.

    Returns:
        list: A list of message objects, formatted for use with the LLM API.
    """

    # Initialize the system message and user prompts from the FEWSHOTS data
    msgs = [{"role": "system", "content": QUERY_SYS}]
    for q, dsl in FEWSHOTS:
        msgs.append({"role": "user", "content": q})
        msgs.append({"role": "assistant", "content": json.dumps(dsl)})

    # Add the user's current query
    msgs.append({"role": "user", "content": nl_query})

    return msgs


def llm_to_dsl(nl_query: str) -> Dict[str, Any]:
    """
    Convert a natural language query into a DSL response using Azure OpenAI.

    Args:
        nl_query (str): The natural language query from the user.

    Returns:
        dict: The generated DSL response parsed from the LLM's output.

    Raises:
        JSONDecodeError: If the response is not valid JSON, an attempt to repair is made.
    """

    # Make a request to the LLM for a response
    resp = client.chat.completions.create(
        model=MODEL_DSL, temperature=0, messages=build_messages(nl_query)
    )
    text = resp.choices[0].message.content.strip()

    # Attempt to parse the response as JSON
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        # If JSONDecodeError occurs, try repairing the response
        fix = client.chat.completions.create(
            model=MODEL_DSL, temperature=0,
            messages=[
              {"role": "system", "content": "Return ONLY valid JSON. Repair the following into a valid JSON object. No comments."},
              {"role": "user", "content": text}
            ]
        )
        return json.loads(fix.choices[0].message.content.strip())


def summarize(nl_query: str, es_response: Dict[str, Any]) -> str:
    """
    Summarize the Elasticsearch query results based on the user's query.

    Args:
        nl_query (str): The natural language query from the user.
        es_response (dict): The response from Elasticsearch containing query results.

    Returns:
        str: The summary generated by the LLM based on the query and results.
    """

    # Prepare the payload for summarization
    payload = {
        "query": nl_query,
        "hits": [h.get("_source", {}) for h in es_response.get("hits", {}).get("hits", [])],
        "aggs": es_response.get("aggregations", {})
    }

    # Request the summary from the LLM
    resp = client.chat.completions.create(
        model=MODEL_SUM, temperature=0,
        messages=[
          {"role": "system", "content": SUMM_SYS},
          {"role": "user", "content": json.dumps(payload)}
        ]
    )

    # Return the generated summary
    return resp.choices[0].message.content.strip()
